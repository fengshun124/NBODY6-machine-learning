# Machine Learning Framework for Star Cluster Properties Prediction

This repository contains a deep learning framework designed to predict properties of star clusters based on variable-size sets of their member stars. This Pytorch-Lightning-based framework implements three different architectures for set-based regression tasks, with a focus on variable-size input handling and permutation invariance.

## Requirements

Python 3.12+ with the following packages (see `requirements.txt` for pinned versions):
- `click`
- `joblib`
- `numpy`
- `pandas`
- `python-dotenv`
- `pytorch_lightning`
- `scikit-learn`
- `torch`
- `torchmetrics`
- `tqdm`

## Usage

### Prepare Dataset

Before training a model, run the following command to preprocess the reduced simulation output generated by `NBODY6-data-pipeline`.

```sh
python ./build_dataset.py
```

### Train a Model

After preparing the dataset, use the following command to train a model. Adjust the parameters as needed.

```sh
# or `env CUDA_VISIBLE_DEVICES=0` if using fish shell
CUDA_VISIBLE_DEVICES=0 python ./train.py \
  # data configuration
  --data cache/9ef03baf1395 \
  # features and target used during training, adjust as needed
  --feature-keys x \
  --feature-keys y \
  --feature-keys z \
  --feature-keys vx \
  --feature-keys vy \
  --feature-keys vz \
  --target-key total_mass_within_2x_r_tidal \
  # model configuration
  --model set_transformer \
  --hparam 'hidden_dim=8' \
  --hparam 'num_heads=8' \
  --hparam 'num_sabs=4' \
  --hparam 'output_hidden_dims=(4,2)' \
  --hparam 'dropout=0.2' \
  # training configuration
  --batch-size 20480 \
  --num-workers 16 \
  -lr 1e-4 \
  -wd 3e-3 \
  --max-epochs 100 \
  --log-dir logs
```

Notes:

- Short aliases are available for convenience: `-lr` for `--learning-rate`, and `-wd` for `--weight-decay`.
- The code expects hyphen-style option names (for example `--feature-keys`, `--target-key`, `--batch-size`).
- Available models: `deep_sets`, `summary_stats`, `set_transformer`.

## Model Architecture

The framework provides three permutation-invariant architectures for variable-size input sets:

- [`SummaryStatsRegressor`](./src/model/summary_stats.py): computes descriptive statistics (e.g., mean, std, quantiles) across set features, then feeds them to an MLP for regression.

- [`DeepSetsRegressor`](./src/model/deep_sets.py): applies a per-element encoder, aggregates via a permutation-invariant pooling (e.g., sum/mean), and decodes with an MLP following the Deep Sets design.

- [`SetTransformerRegressor`](./src/model/set_transformer.py): uses attention-based Set Transformer blocks to model interactions among members before pooling and final MLP decoding.
